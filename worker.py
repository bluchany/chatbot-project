import os
import json
import time
import traceback
from supabase import create_client
from dotenv import load_dotenv

from utils import (
    redis_client,
    get_gemini_embedding,
    MAIN_ANSWER_CACHE_KEY,
    rerank_search_results,
    format_search_results,
    expand_search_query 
)

print("[Worker] ì„¤ì • ë¡œë“œ ì¤‘...")
load_dotenv()

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
JOB_QUEUE_KEY = "chatbot:job_queue"
JOB_RESULTS_KEY = "chatbot:job_results"

print("[Worker] í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
print("[Worker] ì´ˆê¸°í™” ì™„ë£Œ. ì‘ì—… ëŒ€ê¸° ì‹œì‘.")

# --- ì§ˆë¬¸ í™•ì¥ ì‚¬ì „ ---
QUERY_EXPANSION_MAP = {
    "ì¥ì• ê²€ì‚¬": "ì˜ìœ ì•„ ë°œë‹¬ ì •ë°€ ê²€ì‚¬ë¹„ ì§€ì› ì¥ì• ì¸ ë“±ë¡ ì§„ë‹¨ì„œ ë°œê¸‰ë¹„",
    "ë°œë‹¬ê²€ì‚¬": "ì˜ìœ ì•„ ë°œë‹¬ ì •ë°€ ê²€ì‚¬ë¹„ ì§€ì›",
    "ì¹˜ë£Œì§€ì›": "ë°œë‹¬ì¬í™œì„œë¹„ìŠ¤ ë°”ìš°ì²˜ ì§ì¹˜ë£Œ ê·¸ë£¹ì¹˜ë£Œ",
    "ì§ì¹˜ë£Œ": "ë˜ë˜ ê·¸ë£¹ì¹˜ë£Œ ë‘ë¦¬í™œë™ ì‚¬íšŒì„± í–¥ìƒ í”„ë¡œê·¸ë¨ ê·¸ë£¹ í™œë™", 
    "ê·¸ë£¹ì¹˜ë£Œ": "ë˜ë˜ ë‘ë¦¬í™œë™ ì‚¬íšŒì„± í–¥ìƒ í”„ë¡œê·¸ë¨ ì§ì¹˜ë£Œ",
    "ì–¸ì–´ì¹˜ë£Œ": "ë°œë‹¬ì¬í™œì„œë¹„ìŠ¤ ë°”ìš°ì²˜",
    "ë¶€ëª¨êµìœ¡": "ì–‘ìœ¡ ì½”ì¹­ ìƒë‹´",
}

# --- 3. ê²€ìƒ‰ í•¨ìˆ˜ ---
def search_documents_hybrid(query_embedding, keywords, match_count=50):
    try:
        print(f"ğŸ” [Hybrid Search] ì ìš©ëœ í‚¤ì›Œë“œ: {keywords}")
        response = supabase.rpc(
            "match_documents",
            {"query_embedding": query_embedding, "match_count": match_count, "keywords": keywords}
        ).execute()
        return response.data
    except Exception as e:
        print(f"âŒ Supabase ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return []

# --- [ì‹ ê·œ] ë­í‚¹ ë¡œì§ ë¶„ë¦¬ í•¨ìˆ˜ (ì½”ë“œë¥¼ ê¹”ë”í•˜ê²Œ!) ---
def assign_tiers(question, documents):
    """ë¬¸ì„œë“¤ì„ 1í‹°ì–´(ì„±ê³¨), 2í‹°ì–´(ì§„ê³¨), ì¼ë°˜ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    tier_1 = []
    tier_2 = []
    normal = []

    # ì§ˆë¬¸ ìœ í˜• íŒŒì•…
    is_test = "ê²€ì‚¬" in question 
    is_social = any(k in question for k in ["ì§ì¹˜ë£Œ", "ê·¸ë£¹", "ì‚¬íšŒì„±", "ë‘ë¦¬", "ì¹œêµ¬"])
    is_org = any(k in question for k in ["ë¶€ëª¨íšŒ", "ë³µì§€ê´€", "ì„¼í„°", "ë³´ê±´ì†Œ", "ìœ¡ì•„ì¢…í•©"])

    # íŠ¹ìˆ˜ ì§ˆë¬¸ì´ ì•„ë‹ˆë©´ ì „ì²´ë¥¼ ì¼ë°˜ìœ¼ë¡œ ë°˜í™˜
    if not (is_test or is_social or is_org):
        return [], [], documents

    print(f"ğŸ‘®â€â™‚ï¸ [Title Validator] íŠ¹ìˆ˜ ì§ˆë¬¸ ê°ì§€! (ê²€ì‚¬={is_test}, ì‚¬íšŒì„±={is_social}, ê¸°ê´€={is_org})")

    for doc in documents:
        title = doc.get("metadata", {}).get("title", "")
        content = doc.get("content", "")
        
        # --- [ì¡°ê±´ A] ê²€ì‚¬ ì§ˆë¬¸ ---
        if is_test:
            has_test = any(w in title for w in ["ê²€ì‚¬", "ì§„ë‹¨", "ì„ ë³„", "ìŠ¤í¬ë¦¬ë‹", "ë°œë‹¬", "ì •ë°€"])
            has_cost = any(w in title for w in ["ì§€ì›", "ë¹„ìš©", "ë¹„", "ë¬´ë£Œ", "ë°”ìš°ì²˜"])
            
            if has_test and has_cost: tier_1.append(doc)
            elif has_test: tier_2.append(doc)
            else: normal.append(doc)

        # --- [ì¡°ê±´ B] ì§ì¹˜ë£Œ/ì‚¬íšŒì„± ì§ˆë¬¸ ---
        elif is_social:
            if any(w in title for w in ["ë‘ë¦¬", "ì§", "ê·¸ë£¹"]): tier_1.append(doc)
            elif any(w in title for w in ["ì‚¬íšŒì„±", "êµì‹¤", "ì¹œêµ¬"]) or "ë‘ë¦¬" in content: tier_2.append(doc)
            else: normal.append(doc)
        
        # --- [ì¡°ê±´ C] ê¸°ê´€ ì§ˆë¬¸ ---
        elif is_org:
            target_orgs = [k for k in ["ë¶€ëª¨íšŒ", "ë³µì§€ê´€", "ì„¼í„°", "ë³´ê±´ì†Œ", "ìœ¡ì•„ì¢…í•©"] if k in question]
            is_match = False
            for org in target_orgs:
                if org in title or org in content:
                    is_match = True
                    break
            if is_match: tier_1.append(doc)
            else: normal.append(doc)

    print(f"ğŸ‘®â€â™‚ï¸ ë­í‚¹ ë¶„ë¥˜ ì™„ë£Œ: 1í‹°ì–´({len(tier_1)}) > 2í‹°ì–´({len(tier_2)})")
    return tier_1, tier_2, normal


# --- 4. ì‘ì—… ì²˜ë¦¬ í•¨ìˆ˜ (Main) ---
def process_job(job_data):
    start_time = time.time()
    question = job_data.get("question")
    print(f"\nâ–¶ï¸ ì‘ì—… ì‹œì‘: {question}")

    # [Step 1] í‚¤ì›Œë“œ ì „ëµ
    forced_keywords = []
    for trigger, expansion in QUERY_EXPANSION_MAP.items():
        if trigger in question.replace(" ", ""): 
            forced_keywords.extend(expansion.split())
            print(f"âš¡ï¸ [Rule] '{trigger}' ê°ì§€! -> ê°•ì œ í‚¤ì›Œë“œ ì£¼ì…: {forced_keywords}")

    ai_keywords = expand_search_query(question)
    target_keywords = list(dict.fromkeys(forced_keywords + ai_keywords))
    print(f"ğŸ—ï¸ [ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ] {target_keywords}")

    # [Step 2] ê²€ìƒ‰
    embedding_text = f"{question} {' '.join(forced_keywords)}"
    query_embedding = get_gemini_embedding(embedding_text, task_type="RETRIEVAL_QUERY")
    if not query_embedding: return "ì¼ì‹œì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.", [], 0

    raw_results = search_documents_hybrid(query_embedding, target_keywords, match_count=100)
    if not raw_results: return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.", [], 0

    # [ì¤‘ë³µ ì œê±°]
    seen_ids = set()
    unique_results = []
    for doc in raw_results:
        pid = doc.get("metadata", {}).get("page_id")
        if pid not in seen_ids:
            seen_ids.add(pid)
            unique_results.append(doc)
    raw_results = unique_results

    # [Step 2.5] ë¸”ë™ë¦¬ìŠ¤íŠ¸ í•„í„°ë§
    is_medical = "ê²€ì‚¬" in question and not any(w in question for w in ["í•™êµ", "ì…í•™", "êµìœ¡ì²­", "íŠ¹ìˆ˜", "ì„ ë³„"])
    if is_medical:
        raw_results = [d for d in raw_results if not any(x in d.get("metadata", {}).get("title", "") for x in ["íŠ¹ìˆ˜êµìœ¡", "ì„ ë³„", "ë°°ì¹˜", "ì…í•™", "êµìœ¡ì²­"])]

    # [Step 3] ë­í‚¹ ë¶„ë¥˜ (í•¨ìˆ˜ë¡œ ë¶„ë¦¬í•˜ì—¬ ê¹”ë”í•´ì§)
    tier_1_docs, tier_2_docs, normal_docs = assign_tiers(question, raw_results)

    # [Step 4] ë­í‚¹ ì¤€ë¹„ (íŒíŠ¸ ì£¼ì…)
    marked_candidates = []
    for doc in tier_1_docs:
        new_doc = doc.copy()
        new_meta = doc.get("metadata", {}).copy()
        new_meta["title"] = f"â˜…(ìš°ì„ ì¶”ì²œ) {new_meta.get('title')}"
        new_doc["metadata"] = new_meta
        marked_candidates.append(new_doc)
        
    # AI í›„ë³´êµ° (íŒíŠ¸ ë‹¬ë¦° 1í‹°ì–´ + 2í‹°ì–´ + ì¼ë°˜)
    candidates_for_ai = marked_candidates + tier_2_docs + normal_docs

    # [Step 5] AI ë­í‚¹
    print(f"ğŸ¤– Geminiì—ê²Œ {len(candidates_for_ai)}ê°œ ë¬¸ì„œë¥¼ ë³´ëƒ…ë‹ˆë‹¤. (íŒíŠ¸ í¬í•¨)")
    reranked_results = rerank_search_results(question, candidates_for_ai)

    # Fallback: AI ì‹¤íŒ¨ ì‹œ, íŒŒì´ì¬ì´ ì •í•œ ìˆœì„œ(1í‹°ì–´->2í‹°ì–´->ì¼ë°˜) ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if not reranked_results:
        print("âš ï¸ AI ë­í‚¹ ì‹¤íŒ¨ -> íŒŒì´ì¬ ìš°ì„ ìˆœìœ„ ì ìš©")
        reranked_results = candidates_for_ai[:2]

    # [Step 6] ìµœì¢… ê²°ê³¼ ì„ ì • ë° ì¡°ë¦½
    display_count = min(len(reranked_results), 2)
    display_results = reranked_results[:display_count]
    
    # ë” ë³´ê¸°ìš© ID ë¦¬ìŠ¤íŠ¸
    all_page_ids = [r.get("metadata", {}).get("page_id") for r in reranked_results]
    remaining = [d for d in raw_results if d.get("metadata", {}).get("page_id") not in all_page_ids]
    all_page_ids.extend([d.get("metadata", {}).get("page_id") for d in remaining])
    
    # í™”ë©´ í‘œì‹œìš© ë©”íƒ€ë°ì´í„° ì •ì œ (íŒíŠ¸ íƒœê·¸ ì œê±°)
    final_display_metadata = []
    for res in display_results:
        meta = res.get("metadata", {})
        clean_title = meta.get("title", "").replace("â˜…(ìš°ì„ ì¶”ì²œ) ", "")
        meta["title"] = clean_title
        final_display_metadata.append(meta)

    body = format_search_results(final_display_metadata)
    header = "ğŸ” **ì •ë³´ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤!**\nìì„¸í•œ ì •ë³´ëŠ” 'ìì„¸íˆ ë³´ê¸°'ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
    final_answer = f"{header}\n\n<hr>\n\n{body}"

    if len(all_page_ids) > display_count:
        final_answer += f"\n\n<hr>\n\nğŸ” **ì•„ì§ ê²°ê³¼ê°€ ë” ë‚¨ì•„ìˆìŠµë‹ˆë‹¤.**\n'ë” ë³´ì—¬ì¤˜' ë˜ëŠ” 'ë‹¤ìŒ'ì„ ì…ë ¥í•´ ë³´ì„¸ìš”."

    elapsed = time.time() - start_time
    print(f"âœ… ë‹µë³€ ì¡°ë¦½ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {elapsed:.2f}ì´ˆ)")
    return final_answer, all_page_ids, len(all_page_ids)

# --- 5. ë©”ì¸ ë£¨í”„ ---
def start_worker():
    print("ğŸš€ Worker ê°€ë™! Redis í ëŒ€ê¸° ì¤‘...")
    while True:
        try:
            result = redis_client.blpop(JOB_QUEUE_KEY, timeout=0)
            if result:
                _, job_json = result
                job_data = json.loads(job_json.decode('utf-8'))
                job_id = job_data.get("job_id")
                
                answer_text, all_ids, total_found = process_job(job_data)

                final_result = {
                    "status": "complete",
                    "answer": answer_text,
                    "last_result_ids": all_ids, 
                    "total_found": total_found 
                }
                redis_client.hset(JOB_RESULTS_KEY, job_id, json.dumps(final_result).encode('utf-8'))
                print(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ (Job ID: {job_id})")

        except Exception as e:
            print(f"ğŸ”¥ Worker ë£¨í”„ ì˜¤ë¥˜: {e}")
            traceback.print_exc()
            time.sleep(1)

if __name__ == "__main__":
    start_worker()