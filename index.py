import os
import json
import time
import traceback
from supabase import create_client
from notion_client import Client as NotionClient
from dotenv import load_dotenv
from utils import (
    LLM_MODEL,  
    summarize_content_with_llm, 
    _get_title, 
    _get_number, 
    _get_rich_text,
    _get_url,
    get_gemini_embedding,
    _get_multi_select
)

print("[Indexer] ì„¤ì • ë¡œë“œ ì¤‘...")
load_dotenv()

NOTION_KEY = os.getenv("NOTION_KEY")
SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")
if not NOTION_KEY: raise ValueError("NOTION_KEY ì„¤ì • í•„ìš”")
if not SUPABASE_URL or not SUPABASE_KEY: raise ValueError("SUPABASE ì„¤ì • í•„ìš”")

DATABASE_IDS = {
    "ì˜ë£Œ/ì¬í™œ": "2738ade5021080b786b0d8b0c07c1ea2",
    "êµìœ¡/ë³´ìœ¡": "2738ade5021080339203d7148d7d943b",
    "ê°€ì¡± ì§€ì›": "2738ade502108041a4c7f5ec4c3b8413",
    "ëŒë´„/ì–‘ìœ¡": "2738ade5021080cf842df820fdbeb709",
    "ìƒí™œ ì§€ì›": "2738ade5021080579e5be527ff1e80b2"
}
NOTION_PROPERTY_NAMES = {
    "title": "ì‚¬ì—…ëª…", "category": "ë¶„ë¥˜", "sub_category": "ëŒ€ìƒ íŠ¹ì„±",
    "start_age": "ì‹œì‘ ì›”ë ¹(ê°œì›”)", "end_age": "ì¢…ë£Œ ì›”ë ¹(ê°œì›”)", "support_detail": "ìƒì„¸ ì§€ì› ë‚´ìš©",
    "contact": "ë¬¸ì˜ì²˜", "url1": "ê´€ë ¨ í™ˆí˜ì´ì§€ 1", "url2": "ê´€ë ¨ í™ˆí˜ì´ì§€ 2",
    "url3": "ê´€ë ¨ í™ˆí˜ì´ì§€ 3", "extra_req": "ì¶”ê°€ ìê²©ìš”ê±´"
}

STATE_FILE_PATH = "./chroma-data/indexing_state.json"

print("[Indexer] í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì¤‘...")
notion = NotionClient(auth=NOTION_KEY)
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
print("[Indexer] ì´ˆê¸°í™” ì™„ë£Œ.")

def load_state():
    if os.path.exists(STATE_FILE_PATH):
        try:
            with open(STATE_FILE_PATH, "r", encoding="utf-8") as f: return json.load(f)
        except: pass
    return {}

def save_state(state):
    try:
        with open(STATE_FILE_PATH, "w", encoding="utf-8") as f:
            json.dump(state, f, ensure_ascii=False, indent=2)
    except Exception: pass

def run_indexing():
    print("\nğŸ”¥ğŸ”¥ğŸ”¥ [ì½”ë“œ ì—…ë°ì´íŠ¸] ë¬¸ì„œ ì„ë² ë”© ìµœì í™” ëª¨ë“œ (RETRIEVAL_DOCUMENT) ğŸ”¥ğŸ”¥ğŸ”¥\n")
    print("[Indexer] ğŸš€ Supabase ì¦ë¶„ ìƒ‰ì¸ ì‹œì‘...")
    
    if not LLM_MODEL:
        print("âŒ [Indexer] FATAL: Gemini ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨.")
        return

    # prev_state = load_state()  <-- ì´ê±¸ ì£¼ì„ ì²˜ë¦¬í•˜ê³ 
    prev_state = {} 
    current_state = {}
    total_processed = 0
    total_skipped = 0
    has_critical_error = False
    
    for category_name, db_id in DATABASE_IDS.items():
        print(f"\n[Indexer] '{category_name}' DB í™•ì¸ ì¤‘...")
        try:
            results = []
            
            # [ìˆ˜ì • 2] ì•ˆì „í•œ í˜ì´ì§€ë„¤ì´ì…˜(Pagination) ë¡œì§
            has_more = True
            next_cursor = None
            
            while has_more:
                # cursorê°€ ìˆìœ¼ë©´ ë„£ê³ , ì—†ìœ¼ë©´ ëºŒ
                query_params = {"database_id": db_id}
                if next_cursor: query_params["start_cursor"] = next_cursor
                
                response = notion.databases.query(**query_params)
                
                results.extend(response.get("results", []))
                has_more = response.get("has_more")
                next_cursor = response.get("next_cursor")
                time.sleep(0.3) # API ì†ë„ ì œí•œ ì¤€ìˆ˜
            
            print(f" - {len(results)}ê°œ í˜ì´ì§€ ë°œê²¬.")

            for page in results:
                page_id = page.get("id")
                last_edited = page.get("last_edited_time")
                if not page_id: continue
                
                current_state[page_id] = last_edited

                if page_id in prev_state and prev_state[page_id] == last_edited:
                    total_skipped += 1
                    continue

                print(f"\n[Indexer] âš¡ï¸ ì²˜ë¦¬ ì‹œì‘ (ID: {page_id})")

                try:
                    supabase.table("site_pages").delete().eq("page_id", page_id).execute()
                except: pass

                # ë°ì´í„° ì¶”ì¶œ
                props = page.get("properties", {})
                title = _get_title(props, NOTION_PROPERTY_NAMES["title"])
                support_detail = _get_rich_text(props, NOTION_PROPERTY_NAMES["support_detail"])
                extra_req = _get_rich_text(props, NOTION_PROPERTY_NAMES["extra_req"])
                contact = _get_rich_text(props, NOTION_PROPERTY_NAMES["contact"])
                page_url = page.get("url", "")
                start_age = _get_number(props, NOTION_PROPERTY_NAMES["start_age"])
                end_age = _get_number(props, NOTION_PROPERTY_NAMES["end_age"])
                if end_age == -1: end_age = 99999

                targets = _get_multi_select(props, NOTION_PROPERTY_NAMES["sub_category"])
                targets_text = ", ".join(targets) if targets else ""
                
                age_text = ""
                if start_age != -1 and start_age is not None:
                    if end_age != 99999 and end_age is not None: age_text = f"{int(start_age)}~{int(end_age)}ê°œì›”"
                    else: age_text = f"{int(start_age)}ê°œì›” ì´ìƒ"
                elif end_age != 99999 and end_age is not None: age_text = f"~{int(end_age)}ê°œì›”"
                
                final_target = f"{age_text} ({targets_text})" if targets_text else age_text

                # =========================================================
                # [1] ìš”ì•½ìš© í…ìŠ¤íŠ¸ ìƒì„± (ì‚¬ìš©ìì—ê²Œ ë³´ì—¬ì¤„ ì „ì²´ ì •ë³´)
                text_parts = [
                    f"ì‚¬ì—…ëª…: {title}",
                    f"ëŒ€ìƒ: {final_target}",
                    support_detail,
                    f"ì¶”ê°€ ìê²©ìš”ê±´: {extra_req}",
                    f"ë¬¸ì˜ì²˜: {contact}"
                ]
                full_text_for_summary = "\n".join([p.strip() for p in text_parts if p and p.strip()])

                # [2] ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ ìƒì„± (ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒìš©)
                # [â˜…ì „ëµ ìˆ˜ì •â˜…] ì¤‘ìš”ë„ì— ë”°ë¼ ë°˜ë³µ íšŸìˆ˜ë¥¼ ë‹¤ë¥´ê²Œ ì ìš©í•©ë‹ˆë‹¤.
                
                search_keywords = f"{title} {category_name} {targets_text}".replace(" ", ", ")
                req_text = f"ìê²©ìš”ê±´: {extra_req}" if extra_req and extra_req != "â€”" else ""
                
                # ê°€ì¤‘ì¹˜ ì„¤ì • (ë°˜ë³µ íšŸìˆ˜)
                weight_title = 3        # ì œëª©: ì ˆëŒ€ì  ê¸°ì¤€
                weight_target = 2       # ëŒ€ìƒ íŠ¹ì„±: ì¥ì• , ë‹¤ë¬¸í™” ë“± ì¤‘ìš”
                weight_req = 1          # ìê²©ìš”ê±´: ì†Œë“, ê±°ì£¼ì§€ ë“±
                
                # ë¦¬ìŠ¤íŠ¸ ì»´í”„ë¦¬í—¨ì…˜ìœ¼ë¡œ ë°˜ë³µ ìƒì„±
                title_repeats = [f"ë¬¸ì„œì œëª©: {title}" for _ in range(weight_title)]
                target_repeats = [f"ëŒ€ìƒíŠ¹ì„±: {targets_text}" for _ in range(weight_target)] if targets_text else []
                req_repeats = [f"ìê²©ìš”ê±´: {req_text}" for _ in range(weight_req)] if req_text else []
                
                embedding_parts = [
                    f"í•µì‹¬í‚¤ì›Œë“œ: {search_keywords}",
                    f"ì¹´í…Œê³ ë¦¬: {category_name}",
                    f"ëŒ€ìƒ: {final_target}",
                    f"ë‚´ìš©: {support_detail}",
                ] + title_repeats + target_repeats + req_repeats
                
                # (ë‚´ìš© support_detailì€ ë…¸ì´ì¦ˆ ë°©ì§€ë¥¼ ìœ„í•´ ì—¬ì „íˆ ì œì™¸í•©ë‹ˆë‹¤)
                
                full_text_for_embedding = "\n".join([p.strip() for p in embedding_parts if p and p.strip()])
                
                # [â˜…í™•ì¸ìš©â˜…] 
                if total_processed == 0: 
                     print(f"ğŸ” [X-RAY] ê°€ì¤‘ì¹˜ ì ìš©ëœ ê²€ìƒ‰ ë°ì´í„° ì˜ˆì‹œ:\n{full_text_for_embedding[:300]}...")
                # =========================================================

                # =========================================================
                
                # í˜ì´ì§€ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬
                chunks = [full_text_for_summary] 
                records_to_insert = []
                
                for i, chunk_text in enumerate(chunks):
                    if len(chunk_text.strip()) < 10: continue
                    chunk_id = f"{page_id}_{i}"

                    print(f"[Indexer] ... '{title}' ìš”ì•½ ë° ì„ë² ë”© ì¤‘...")
                    
                    # 1. ìš”ì•½
                    pre_summary = summarize_content_with_llm(chunk_text, title, [])

                    # 2. ì„ë² ë”© [â˜…ìˆ˜ì • 1â˜…] ë¬¸ì„œ ì €ì¥ìš© íƒœìŠ¤í¬ íƒ€ì… ì‚¬ìš©!
                    # ê²€ìƒ‰í•  ë•Œ(Query)ì™€ ì €ì¥í•  ë•Œ(Document)ì˜ íƒ€ì…ì´ ë‹¬ë¼ì•¼ ì •í™•ë„ê°€ ì˜¬ë¼ê°‘ë‹ˆë‹¤.
                    embedding = get_gemini_embedding(
                        full_text_for_embedding, 
                        task_type="RETRIEVAL_DOCUMENT" # <--- í•µì‹¬ ìˆ˜ì •!
                    )

                    if not embedding:
                        print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨! ê±´ë„ˆëœ€.")
                        continue

                    metadata = {
                        "page_id": page_id,
                        "category": category_name,
                        "sub_category_list": targets, # [â˜…ìˆ˜ì • 3] ë¦¬ìŠ¤íŠ¸ ì›ë³¸ ì €ì¥ (í•„í„°ë§ìš©)
                        "start_age": start_age,
                        "end_age": end_age,
                        "title": title,
                        "page_url": page_url,
                        "pre_summary": pre_summary
                    }

                    records_to_insert.append({
                        "id": chunk_id,
                        "page_id": page_id,
                        "content": full_text_for_summary, # DBì—ëŠ” ì „ì²´ ë‚´ìš© ì €ì¥
                        "metadata": metadata,
                        "embedding": embedding # ë²¡í„°ëŠ” í•µì‹¬ ë‚´ìš©ìœ¼ë¡œë§Œ ê³„ì‚°
                    })

                if records_to_insert:
                    try:
                        supabase.table("site_pages").upsert(records_to_insert).execute()
                        total_processed += 1
                    except Exception as e:
                        print(f"âŒ ì €ì¥ ì‹¤íŒ¨: {e}")

        except Exception as e:
            print(f"âŒ ì˜¤ë¥˜ ({category_name}): {e}")
            traceback.print_exc()
            has_critical_error = True

    # ì‚­ì œ ì²˜ë¦¬ ë¡œì§
    if has_critical_error:
        print("\n[Indexer] âš ï¸ ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì‚­ì œ ë‹¨ê³„ ê±´ë„ˆëœ€.")
    else:
        deleted_ids = list(set(prev_state.keys()) - set(current_state.keys()))
        if deleted_ids:
            print(f"\n[Indexer] ğŸ—‘ï¸ ì‚­ì œëœ í˜ì´ì§€ {len(deleted_ids)}ê±´ ì •ë¦¬ ì¤‘...")
            for del_id in deleted_ids:
                try:
                    supabase.table("site_pages").delete().eq("page_id", del_id).execute()
                except: pass
        
        save_state(current_state)
        print(f"\n[Indexer] âœ¨ ì™„ë£Œ. (ì—…ë°ì´íŠ¸: {total_processed}, ê±´ë„ˆëœ€: {total_skipped})")

if __name__ == "__main__":
    run_indexing()